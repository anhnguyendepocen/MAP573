<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>R for Statistics</title>
  <meta name="description" content="R for Statistics">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="R for Statistics" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="R for Statistics" />
  
  
  

<meta name="author" content="Julie Josse">


<meta name="date" content="2017-11-07">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="homework-pca.html">
<link rel="next" href="shiny-app.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R for Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Syllabus</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#in-a-nutshell"><i class="fa fa-check"></i><b>1.1</b> In a nutshell</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#outline"><i class="fa fa-check"></i><b>1.2</b> Outline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="what-is-r.html"><a href="what-is-r.html"><i class="fa fa-check"></i><b>2</b> What is R?</a></li>
<li class="chapter" data-level="3" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html"><i class="fa fa-check"></i><b>3</b> Getting Started with R</a><ul>
<li class="chapter" data-level="3.1" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#download-r-and-r-studio"><i class="fa fa-check"></i><b>3.1</b> Download R and R Studio</a></li>
<li class="chapter" data-level="3.2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#objects"><i class="fa fa-check"></i><b>3.2</b> Objects</a></li>
<li class="chapter" data-level="3.3" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#section"><i class="fa fa-check"></i><b>3.3</b> “&gt;”, “&lt;”, “&gt;=”, “&lt;=”, “==”, “!=”</a></li>
<li class="chapter" data-level="3.4" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#programming"><i class="fa fa-check"></i><b>3.4</b> Programming</a><ul>
<li class="chapter" data-level="3.4.1" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#practice"><i class="fa fa-check"></i><b>3.4.1</b> Practice</a></li>
<li class="chapter" data-level="3.4.2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#distributions"><i class="fa fa-check"></i><b>3.4.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#import"><i class="fa fa-check"></i><b>3.5</b> Import data</a></li>
<li class="chapter" data-level="3.6" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#first-descriptive-statistics"><i class="fa fa-check"></i><b>3.6</b> First descriptive statistics</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="reproducible.html"><a href="reproducible.html"><i class="fa fa-check"></i><b>4</b> Reproducible research</a></li>
<li class="chapter" data-level="5" data-path="graphics.html"><a href="graphics.html"><i class="fa fa-check"></i><b>5</b> Graphics</a><ul>
<li class="chapter" data-level="5.1" data-path="graphics.html"><a href="graphics.html#classical-plots"><i class="fa fa-check"></i><b>5.1</b> Classical plots</a><ul>
<li class="chapter" data-level="5.1.1" data-path="graphics.html"><a href="graphics.html#representing-1-variables"><i class="fa fa-check"></i><b>5.1.1</b> Representing 1 variables</a></li>
<li class="chapter" data-level="5.1.2" data-path="graphics.html"><a href="graphics.html#reprensenting-2-variables"><i class="fa fa-check"></i><b>5.1.2</b> Reprensenting 2 variables</a></li>
<li class="chapter" data-level="5.1.3" data-path="graphics.html"><a href="graphics.html#saving-graphical-outputs"><i class="fa fa-check"></i><b>5.1.3</b> Saving graphical outputs</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="graphics.html"><a href="graphics.html#ggplot2"><i class="fa fa-check"></i><b>5.2</b> Ggplot2</a><ul>
<li class="chapter" data-level="5.2.1" data-path="graphics.html"><a href="graphics.html#scatter-plots"><i class="fa fa-check"></i><b>5.2.1</b> Scatter plots</a></li>
<li class="chapter" data-level="5.2.2" data-path="graphics.html"><a href="graphics.html#facets"><i class="fa fa-check"></i><b>5.2.2</b> facets</a></li>
<li class="chapter" data-level="5.2.3" data-path="graphics.html"><a href="graphics.html#more-options"><i class="fa fa-check"></i><b>5.2.3</b> more options</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="graphics.html"><a href="graphics.html#first-interactive-graphs"><i class="fa fa-check"></i><b>5.3</b> First interactive graphs</a></li>
<li class="chapter" data-level="5.4" data-path="graphics.html"><a href="graphics.html#spatial-graphs"><i class="fa fa-check"></i><b>5.4</b> Spatial graphs</a></li>
<li class="chapter" data-level="5.5" data-path="graphics.html"><a href="graphics.html#going-further"><i class="fa fa-check"></i><b>5.5</b> Going further</a><ul>
<li class="chapter" data-level="5.5.1" data-path="graphics.html"><a href="graphics.html#cross-validation"><i class="fa fa-check"></i><b>5.5.1</b> Cross-validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html"><i class="fa fa-check"></i><b>6</b> Homework: exploratory analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#wine-tasting"><i class="fa fa-check"></i><b>6.1</b> Wine tasting</a></li>
<li class="chapter" data-level="6.2" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#hospital"><i class="fa fa-check"></i><b>6.2</b> Management of severe trauma patients</a><ul>
<li class="chapter" data-level="6.2.1" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#question-1---distribution-of-missing-data"><i class="fa fa-check"></i><b>6.2.1</b> Question 1 - distribution of missing data</a></li>
<li class="chapter" data-level="6.2.2" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#question-2---dealing-with-missing-data"><i class="fa fa-check"></i><b>6.2.2</b> Question 2 - dealing with missing data</a></li>
<li class="chapter" data-level="6.2.3" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#question-3---exploration-of-the-data"><i class="fa fa-check"></i><b>6.2.3</b> Question 3 - exploration of the data</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#map-of-chicago---prediction-of-chicago-criminality"><i class="fa fa-check"></i><b>6.3</b> Map of Chicago - Prediction of Chicago criminality</a></li>
<li class="chapter" data-level="6.4" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#installing-ggmap"><i class="fa fa-check"></i><b>6.4</b> Installing ggmap</a></li>
<li class="chapter" data-level="6.5" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#load-the-data"><i class="fa fa-check"></i><b>6.5</b> Load the data</a></li>
<li class="chapter" data-level="6.6" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#set-your-working-directory"><i class="fa fa-check"></i><b>6.6</b> Set your working directory</a></li>
<li class="chapter" data-level="6.7" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#load-the-dataset"><i class="fa fa-check"></i><b>6.7</b> load the dataset</a></li>
<li class="chapter" data-level="6.8" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#question-1---first-map"><i class="fa fa-check"></i><b>6.8</b> Question 1 - First map</a></li>
<li class="chapter" data-level="6.9" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#load-the-map-of-chicago"><i class="fa fa-check"></i><b>6.9</b> Load the map of Chicago</a></li>
<li class="chapter" data-level="6.10" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#code-to-answer-the-questions-of-the-homework"><i class="fa fa-check"></i><b>6.10</b> Code to answer the questions of the homework</a></li>
<li class="chapter" data-level="6.11" data-path="homework-exploratory-analysis.html"><a href="homework-exploratory-analysis.html#question-2---density-plots"><i class="fa fa-check"></i><b>6.11</b> Question 2 - Density plots</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tests.html"><a href="tests.html"><i class="fa fa-check"></i><b>7</b> Tests</a><ul>
<li class="chapter" data-level="7.1" data-path="tests.html"><a href="tests.html#confidence-intervals-for-a-mean"><i class="fa fa-check"></i><b>7.1</b> Confidence intervals for a mean</a></li>
<li class="chapter" data-level="7.2" data-path="tests.html"><a href="tests.html#mean-comparison"><i class="fa fa-check"></i><b>7.2</b> Mean comparison</a><ul>
<li class="chapter" data-level="7.2.1" data-path="tests.html"><a href="tests.html#testing-the-equality-of-variances"><i class="fa fa-check"></i><b>7.2.1</b> Testing the equality of variances</a></li>
<li class="chapter" data-level="7.2.2" data-path="tests.html"><a href="tests.html#testing-the-equality-of-means"><i class="fa fa-check"></i><b>7.2.2</b> Testing the equality of means</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="tests.html"><a href="tests.html#some-comments-on-p-values"><i class="fa fa-check"></i><b>7.3</b> Some comments on <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="7.4" data-path="tests.html"><a href="tests.html#power"><i class="fa fa-check"></i><b>7.4</b> Power</a></li>
<li class="chapter" data-level="7.5" data-path="tests.html"><a href="tests.html#chi-square-tests"><i class="fa fa-check"></i><b>7.5</b> Chi-square tests</a><ul>
<li class="chapter" data-level="7.5.1" data-path="tests.html"><a href="tests.html#goodness-of-fit"><i class="fa fa-check"></i><b>7.5.1</b> Goodness of fit</a></li>
<li class="chapter" data-level="7.5.2" data-path="tests.html"><a href="tests.html#independence-test"><i class="fa fa-check"></i><b>7.5.2</b> Independence test</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="tests.html"><a href="tests.html#MultipleTest"><i class="fa fa-check"></i><b>7.6</b> Multiple testing</a></li>
<li class="chapter" data-level="7.7" data-path="tests.html"><a href="tests.html#correlations-spurious-causality-be-careful"><i class="fa fa-check"></i><b>7.7</b> Correlations spurious? causality? be careful!</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="homework-tests.html"><a href="homework-tests.html"><i class="fa fa-check"></i><b>8</b> Homework: tests</a><ul>
<li class="chapter" data-level="8.1" data-path="homework-tests.html"><a href="homework-tests.html#simulations-confidence-interval-for-mu"><i class="fa fa-check"></i><b>8.1</b> Simulations: confidence interval for <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="8.2" data-path="homework-tests.html"><a href="homework-tests.html#missing-values-introduction"><i class="fa fa-check"></i><b>8.2</b> Missing values, introduction</a></li>
<li class="chapter" data-level="8.3" data-path="homework-tests.html"><a href="homework-tests.html#microarray-analysis"><i class="fa fa-check"></i><b>8.3</b> Microarray analysis</a></li>
<li class="chapter" data-level="8.4" data-path="homework-tests.html"><a href="homework-tests.html#chi-squares-test---goodness-of-fit."><i class="fa fa-check"></i><b>8.4</b> Chi-squares test - Goodness of fit.</a></li>
<li class="chapter" data-level="8.5" data-path="homework-tests.html"><a href="homework-tests.html#rv-and-dcov-coefficients"><i class="fa fa-check"></i><b>8.5</b> RV and dCov coefficients</a><ul>
<li class="chapter" data-level="8.5.1" data-path="homework-tests.html"><a href="homework-tests.html#expected-values-of-the-rv-coefficient-with-respect-to-the-dimensions"><i class="fa fa-check"></i><b>8.5.1</b> Expected values of the RV coefficient with respect to the dimensions</a></li>
<li class="chapter" data-level="8.5.2" data-path="homework-tests.html"><a href="homework-tests.html#permutation-test-for-wine-data"><i class="fa fa-check"></i><b>8.5.2</b> Permutation test for wine data</a></li>
<li class="chapter" data-level="8.5.3" data-path="homework-tests.html"><a href="homework-tests.html#power-of-the-rv-and-dcov-tests-in-the-case-of-a-linear-relation"><i class="fa fa-check"></i><b>8.5.3</b> Power of the RV and dCov tests in the case of a linear relation</a></li>
<li class="chapter" data-level="8.5.4" data-path="homework-tests.html"><a href="homework-tests.html#power-of-the-rv-and-dcov-tests-in-the-case-of-a-nonlinear-relation"><i class="fa fa-check"></i><b>8.5.4</b> Power of the RV and dCov tests in the case of a nonlinear relation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html"><i class="fa fa-check"></i><b>9</b> Principal components analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#lecture"><i class="fa fa-check"></i><b>9.1</b> Lecture</a></li>
<li class="chapter" data-level="9.2" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#detailed-derivation-of-principal-component-analysis"><i class="fa fa-check"></i><b>9.2</b> Detailed derivation of Principal Component Analysis</a><ul>
<li class="chapter" data-level="9.2.1" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#maximizing-the-variance"><i class="fa fa-check"></i><b>9.2.1</b> Maximizing the variance</a></li>
<li class="chapter" data-level="9.2.2" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#minimizing-the-reconstruction-error"><i class="fa fa-check"></i><b>9.2.2</b> Minimizing the reconstruction error</a></li>
<li class="chapter" data-level="9.2.3" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#relationship-with-svd"><i class="fa fa-check"></i><b>9.2.3</b> Relationship with SVD</a></li>
<li class="chapter" data-level="9.2.4" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#explained-variance-and-correlation"><i class="fa fa-check"></i><b>9.2.4</b> Explained variance and correlation</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#lab"><i class="fa fa-check"></i><b>9.3</b> Lab</a><ul>
<li class="chapter" data-level="9.3.1" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#lecture-questions"><i class="fa fa-check"></i><b>9.3.1</b> Lecture questions</a></li>
<li class="chapter" data-level="9.3.2" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#analysis-of-decathlon-data"><i class="fa fa-check"></i><b>9.3.2</b> Analysis of decathlon data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="homework-pca.html"><a href="homework-pca.html"><i class="fa fa-check"></i><b>10</b> Homework: PCA</a><ul>
<li class="chapter" data-level="10.1" data-path="homework-pca.html"><a href="homework-pca.html#pca-on-random-data"><i class="fa fa-check"></i><b>10.1</b> PCA on random data</a></li>
<li class="chapter" data-level="10.2" data-path="homework-pca.html"><a href="homework-pca.html#pca-by-hand"><i class="fa fa-check"></i><b>10.2</b> PCA by hand</a></li>
<li class="chapter" data-level="10.3" data-path="homework-pca.html"><a href="homework-pca.html#other-applications-to-practice"><i class="fa fa-check"></i><b>10.3</b> Other applications to practice</a><ul>
<li class="chapter" data-level="10.3.1" data-path="homework-pca.html"><a href="homework-pca.html#ozone-data"><i class="fa fa-check"></i><b>10.3.1</b> Ozone data</a></li>
<li class="chapter" data-level="10.3.2" data-path="homework-pca.html"><a href="homework-pca.html#mice-microarray-data-and-fatty-acid-concentration"><i class="fa fa-check"></i><b>10.3.2</b> Mice microarray data and fatty acid concentration</a></li>
<li class="chapter" data-level="10.3.3" data-path="homework-pca.html"><a href="homework-pca.html#chicken-microarray-data"><i class="fa fa-check"></i><b>10.3.3</b> Chicken microarray data</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="homework-pca.html"><a href="homework-pca.html#low-rank-matrix-approximation---reconstruction-with-pca"><i class="fa fa-check"></i><b>10.4</b> Low rank matrix approximation - reconstruction with PCA</a></li>
<li class="chapter" data-level="10.5" data-path="homework-pca.html"><a href="homework-pca.html#reconstruct-an-image"><i class="fa fa-check"></i><b>10.5</b> Reconstruct an image</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="handling-missing-values.html"><a href="handling-missing-values.html"><i class="fa fa-check"></i><b>11</b> Handling missing values</a><ul>
<li class="chapter" data-level="11.1" data-path="handling-missing-values.html"><a href="handling-missing-values.html#lecture-missing-values"><i class="fa fa-check"></i><b>11.1</b> Lecture Missing values</a><ul>
<li class="chapter" data-level="11.1.1" data-path="handling-missing-values.html"><a href="handling-missing-values.html#introduction"><i class="fa fa-check"></i><b>11.1.1</b> Introduction</a></li>
<li class="chapter" data-level="11.1.2" data-path="handling-missing-values.html"><a href="handling-missing-values.html#expectation-maximization-algorithm"><i class="fa fa-check"></i><b>11.1.2</b> Expectation Maximization algorithm</a></li>
<li class="chapter" data-level="11.1.3" data-path="handling-missing-values.html"><a href="handling-missing-values.html#conditional-distributions-in-the-gaussian-case"><i class="fa fa-check"></i><b>11.1.3</b> Conditional distributions in the Gaussian case</a></li>
<li class="chapter" data-level="11.1.4" data-path="handling-missing-values.html"><a href="handling-missing-values.html#bootstrap"><i class="fa fa-check"></i><b>11.1.4</b> Bootstrap</a></li>
<li class="chapter" data-level="11.1.5" data-path="handling-missing-values.html"><a href="handling-missing-values.html#gibbs-sampling"><i class="fa fa-check"></i><b>11.1.5</b> Gibbs sampling</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="handling-missing-values.html"><a href="handling-missing-values.html#lab-1"><i class="fa fa-check"></i><b>11.2</b> Lab</a><ul>
<li class="chapter" data-level="11.2.1" data-path="handling-missing-values.html"><a href="handling-missing-values.html#lecture-questions-1"><i class="fa fa-check"></i><b>11.2.1</b> Lecture questions</a></li>
<li class="chapter" data-level="11.2.2" data-path="handling-missing-values.html"><a href="handling-missing-values.html#continuous-data-with-missing-values---regression-with-missing-data-via-multiple-imputation"><i class="fa fa-check"></i><b>11.2.2</b> Continuous data with missing values - Regression with missing data via Multiple Imputation</a></li>
<li class="chapter" data-level="11.2.3" data-path="handling-missing-values.html"><a href="handling-missing-values.html#em-algorithm-for-bivariate-normal-data-with-missing-values"><i class="fa fa-check"></i><b>11.2.3</b> EM algorithm for bivariate normal data with missing values</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="shiny-app.html"><a href="shiny-app.html"><i class="fa fa-check"></i><b>12</b> Shiny App</a><ul>
<li class="chapter" data-level="12.1" data-path="shiny-app.html"><a href="shiny-app.html#lecture-1"><i class="fa fa-check"></i><b>12.1</b> Lecture</a></li>
<li class="chapter" data-level="12.2" data-path="shiny-app.html"><a href="shiny-app.html#lab-representation-of-the-results-of-election-in-usa-since-1789"><i class="fa fa-check"></i><b>12.2</b> Lab: Representation of the results of election in USA since 1789</a></li>
<li class="chapter" data-level="12.3" data-path="shiny-app.html"><a href="shiny-app.html#read-in-data-and-organize"><i class="fa fa-check"></i><b>12.3</b> READ IN DATA AND ORGANIZE</a></li>
<li class="chapter" data-level="12.4" data-path="shiny-app.html"><a href="shiny-app.html#load-packages"><i class="fa fa-check"></i><b>12.4</b> LOAD PACKAGES</a></li>
<li class="chapter" data-level="12.5" data-path="shiny-app.html"><a href="shiny-app.html#make-map"><i class="fa fa-check"></i><b>12.5</b> MAKE MAP</a></li>
<li class="chapter" data-level="12.6" data-path="shiny-app.html"><a href="shiny-app.html#make-table-summary-of-election-result"><i class="fa fa-check"></i><b>12.6</b> MAKE TABLE SUMMARY OF ELECTION RESULT</a></li>
<li class="chapter" data-level="12.7" data-path="shiny-app.html"><a href="shiny-app.html#make-sentence-summary-of-election-result"><i class="fa fa-check"></i><b>12.7</b> MAKE SENTENCE SUMMARY OF ELECTION RESULT</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ref.html"><a href="ref.html"><i class="fa fa-check"></i><b>13</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R for Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="handling-missing-values" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Handling missing values</h1>
<div id="lecture-missing-values" class="section level2">
<h2><span class="header-section-number">11.1</span> Lecture Missing values</h2>
<div id="introduction" class="section level3">
<h3><span class="header-section-number">11.1.1</span> Introduction</h3>
<p>The problem of missing values exists since the earliest attempts of exploiting data as a source of knowledge, as it lies intrinsically in the process of obtaining, recording, and preparation of the data itself. Clearly, (citing Gertrude Mary Cox) “The best thing to do with missing values is not to have any”, but in the contemporary world, considering the increasingly growing amount of accessible data and demand in statistical justification this is not always the case, nay never. Main references on missing values include Schafer (1997), Little and Rubin (1987, 2002), van Buuren (2012), Carpenter and Kenward (2013) and (Gelman and Hill, 2007)[chp. 25].</p>
<p>Missing values occur for plenty of reasons: machines that fail, individuals who forget or do not want to answer to some questions of a questionnaire, damaged plants, etc. They are problematic since most statistical methods cannot be applied on a incomplete dataset. In this chapter we review the different types of missing data and statistical methods which allow their incorporation.</p>
<div id="mcar-mar-mnar" class="section level4">
<h4><span class="header-section-number">11.1.1.1</span> MCAR, MAR, MNAR</h4>
<p>There are several types of missing data, and explaining the reasons why part of the data is missing is crucial to perform inference or any kind of statistical analysiss. Dealing with missing data boils down to considering that the observed data <span class="math inline">\(X_{\text{OBS}}\)</span> is only a subset of a complete data model <span class="math inline">\(X = (X_{\text{OBS}},X_{\text{MIS}})\)</span> which is not fully observable (i.e. <span class="math inline">\(X_{\text{MIS}}\)</span> are the missing data). Assume <span class="math inline">\(X = (X_1,\ldots,X_n)\)</span>; the missing values <span class="math inline">\(X_{\text{MIS}}\)</span> are characterized by a set of indices <span class="math inline">\(I_{\text{MIS}}\subset \{1,\ldots,n\}\)</span> such that <span class="math inline">\(X_{\text{MIS}} = \{X_i; i\in I_{\text{MIS}}\}\)</span>. We define the indicator of missingness <span class="math inline">\(M \in \{0,1\}^n\)</span> such that <span class="math inline">\(M_i = 1\)</span> if <span class="math inline">\(i\in I_{\text{MIS}}\)</span> and <span class="math inline">\(M_i = 0\)</span> otherwise; <span class="math inline">\(M\)</span> defines the  of missingness. Both <span class="math inline">\(X\)</span> and <span class="math inline">\(M\)</span> are modeled as random variables with probability distributions <span class="math inline">\(\mathbb{P}_X\)</span> and <span class="math inline">\(\mathbb{P}_M\)</span> respectively. The different types of missing data refer to different dependence relationships between <span class="math inline">\(X_{\text{OBS}},X_{\text{MIS}}\)</span> and <span class="math inline">\(M\)</span>.</p>
<p>The observations are said to be <strong>Missing Completely At Random (MCAR)</strong> if the probability that an observation is missing is independent of the variables and observations in the dataset: the probability that an observation is missing does not depend on <span class="math inline">\((X_{\text{OBS}},X_{\text{MIS}})\)</span>. Formally,</p>
<p><span class="math display">\[\mathbb{P}_M(M|X_{\text{OBS}},X_{\text{MIS}}) =  \mathbb{P}_M(M).\]</span></p>
<p>The observations are said to be <strong>missing at random (MAR)</strong> if the probability that an observation is missing only depends on the observed data <span class="math inline">\(X_{\text{OBS}}\)</span>. Formally,</p>
<p><span class="math display">\[\mathbb{P}_M(M|X_{\text{OBS}},X_{\text{MIS}}) =  \mathbb{P}_M(M|X_{\text{OBS}}).\]</span></p>
<p>The observations are said to be <strong>Missing Not At Random (MNAR)</strong> in all other cases.</p>
</div>
<div id="ignorable-missing-values" class="section level4">
<h4><span class="header-section-number">11.1.1.2</span> Ignorable missing values</h4>
<p>Many statistical methods are based on estimating a parameter by maximizing the likelihood of the data. Assume that <span class="math inline">\(X\)</span> has a density, parametrized by some parameter <span class="math inline">\(\theta\)</span> that we want to estimate - if <span class="math inline">\(X\)</span> is Gaussian for instance we simply have <span class="math inline">\(\theta = (\mu, \Sigma)\)</span>. Assume that <span class="math inline">\(M\)</span> also has a density parametrized by another parameter <span class="math inline">\(\phi\)</span> - for example the probability <span class="math inline">\(p\)</span> of a Bernoulli distribution. In some cases, estimating <span class="math inline">\(\theta\)</span> from an incomplete data can be done in a very simple way by <em>ignoring</em>, or “skipping” the missing data, as detailed below.</p>
<p>We denote by <span class="math inline">\(f(X,M|\theta, \phi)\)</span> the joint density of the observed and missing entries and of the indicator of missingness conditioned on parameters <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span>. In the context of maximum likelihood estimation, we maximize with respect to <span class="math inline">\(\theta\)</span> the marginal density of the observed data <span class="math inline">\(X_{\text{OBS}}\)</span> <span class="math display">\[f(X_{\text{OBS}},M|\theta, \phi) = \int f(X_{\text{OBS}}, X_{\text{MIS}},M|\theta, \phi)dX_{\text{MIS}}.\]</span> If the data are <strong>MAR</strong> (or <strong>MCAR</strong>), the following factorization holds <span class="math display">\[f(X_{\text{OBS}}, X_{\text{MIS}},M|\theta, \phi) = f(X_{\text{OBS}}, X_{\text{MIS}}|\theta) f(M|X_{\text{OBS}}, \phi).\]</span> Plugging this in the expression of the marginal density we obtain <span class="math display">\[f(X_{\text{OBS}},M|\theta, \phi) = \int f(X_{\text{OBS}}, X_{\text{MIS}}|\theta) f(M|X_{\text{OBS}}, \phi)dX_{\text{MIS}},\]</span></p>
<p><span class="math display">\[f(X_{\text{OBS}},M|\theta, \phi) = f(M|X_{\text{OBS}}, \phi)\int f(X_{\text{OBS}}, X_{\text{MIS}}|\theta) dX_{\text{MIS}},\]</span> <span class="math display">\[f(X_{\text{OBS}},M|\theta, \phi) = f(M|X_{\text{OBS}}, \phi) f(X_{\text{OBS}}|\theta) .\]</span> If <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\theta\)</span> are <em>distinct</em> (the joint parameter space of (<span class="math inline">\(\theta\)</span>, <span class="math inline">\(\phi\)</span>) is the product of the parameter space of <span class="math inline">\(\theta\)</span> and the parameter space of <span class="math inline">\(\phi\)</span>), as the term <span class="math inline">\(f(M|X_{\text{OBS}}, \phi)\)</span> is respect to <span class="math inline">\(\theta\)</span>, it is equivalent to maximize the likelihood <span class="math inline">\(f(X_{\text{OBS}}|\theta)\)</span>, i.e. to <em>ignore</em> the missing data. It really means that when doing inference, i.e. to get the ML estimates for parameters from an incomplete set, one can “simply” maximizes the observed likelihood while ignoring the process that have generated missing values. Consequently, most of the methods used in practice relie on the assumption that the data are <strong>MAR</strong>.</p>
</div>
<div id="two-recommended-methods-em-multiple-imputation" class="section level4">
<h4><span class="header-section-number">11.1.1.3</span> Two recommended methods: EM / Multiple imputation</h4>
<p>Under the classical missing at random mechanism (MAR) assumption, the parameters can thus be estimated by maximizing the observed likelihood. To do so, it is possible to use an Expectation-Maximization (EM) algorithm (Dempster, Laird, and Rubin, 1977) as detailled in the next paragraph - The standard error of the parameters can be estimated using a supplemented Expectation-Maximization (SEM) algorithm (Meng and Rubin, 1991). This is the first main startegy to do inference with missing values. In fact, it consists in adapting the statistical analysis (the estimation process) so that it can be applied on an incomplete data set. It is tailored to a specific statistical method but it has two drawbacks: 1) it can be difficult to establish; 2) a specific algorithm has to be derived for each statistical method that we would like to apply.</p>
<p>This is why the second strategy, namely multiple imputation (MI) (Rubin, 1987, Little and Rubin, 1987, 2002) seems to have taken the lead. The principle of MI consists in predicting <span class="math inline">\(M\)</span> different values for each missing value, which leads to M imputed data sets. The variability across the imputations reflects the variance of the prediction of each missing entry. Then, MI consists in performing the statistical analysis on each imputed data set to estimate the parameter <span class="math inline">\(\theta\)</span> and consists of combining the results <span class="math inline">\((\theta_m)_{1≤m≤M}\)</span> to provide a unique estimation for <span class="math inline">\(\theta\)</span> and for its associated variability using Rubin’s rules (Rubin, 1987). This ensures that the variance of the estimator is not underestimated and thus good coverage properties. What is important is that the aim of both approaches is to estimate as well as possible the parameters and their variance despite missing values, i.e. taking into account the supplementary variability due to missing values. The goal is not to impute the entries as accurately as possible.</p>
</div>
</div>
<div id="expectation-maximization-algorithm" class="section level3">
<h3><span class="header-section-number">11.1.2</span> Expectation Maximization algorithm</h3>
<p>In the case where we are interested in estimating some unknown parameter <span class="math inline">\(\theta\in\mathbb{R}^d\)</span> characterizing the model (such as <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> in the Gaussian example), the Expectation Maximization (EM) algorithm (Dempster et al. 1977) can be used when the joint distribution of the missing data <span class="math inline">\(X_{\text{MIS}}\)</span> and the observed data <span class="math inline">\(X_{\text{OBS}}\)</span> is explicit. For all <span class="math inline">\(\theta\in\mathbb{R}^d\)</span> let <span class="math inline">\(f_{\theta}\)</span> be the probability density function of <span class="math inline">\((X_{\text{OBS}},X_{\text{MIS}})\)</span> with respect to a given reference measure <span class="math inline">\(\mu\)</span>. The EM algorithm aims at iteratively maximizing the likelihood of the observations, i.e. the probability density function of the observations, where <span class="math inline">\(y\)</span> refers to <span class="math inline">\(X_{\text{OBS}}\)</span> and <span class="math inline">\(x\)</span> to <span class="math inline">\(X_{\text{MIS}}\)</span>: <span class="math display">\[
L_{\theta}(y) = \int f_{\theta}(x,y)\lambda(\mathrm{d}x)\,.
\]</span> As this quantity cannot be computed explicitly in general cases, the EM algorithm relies on the surrogate intermediate quantity: <span class="math display">\[
Q(\theta,\theta&#39;) =\mathbb{E}_{\theta&#39;}[\log f_{\theta}(X_{\text{OBS}},X_{\text{MIS}})|X_{\text{OBS}}]\,,
\]</span> where <span class="math inline">\(\mathbb{E}_{\theta&#39;}\)</span> is the expectation under the law of the model parameterized by <span class="math inline">\(\theta&#39;\)</span>. The following crucial property motivates the EM algorithm: for all <span class="math inline">\(\theta,\theta&#39;\)</span>, <span class="math display">\[
\log L_{\theta}(Y) - \log L_{\theta&#39;}(Y) \ge Q(\theta,\theta&#39;)-Q(\theta&#39;,\theta&#39;)\,.
\]</span> Therefore, any value <span class="math inline">\(\theta\)</span> such that <span class="math inline">\(Q(\theta,\theta&#39;)\)</span> is greater than the reference value <span class="math inline">\(Q(\theta&#39;,\theta&#39;)\)</span> increases the loglikelihood of the observations. Based on this inequality, the EM algorithm produces iteratively a sequence of parameter estimates <span class="math inline">\((\theta_p)_{p\ge 0}\)</span>. Each iteration is decomposed into two steps: <span class="math display">\[
\mbox{E-step: compute} \quad \theta \mapsto Q(\theta,\theta_p)\,,\\
\mbox{M-step: set} \quad \theta_p\in \mbox{Argmax}_\theta Q(\theta,\theta_p)\,.
\]</span> The practical interest of this algorithm can be assessed only in cases where <span class="math inline">\(Q(\theta,\theta_p)\)</span> can be computed (or estimated) with a reasonable computational cost (see for instance the special case where <span class="math inline">\(f_{\theta}\)</span> belongs to the exponential family) and when <span class="math inline">\(\theta \mapsto Q(\theta,\theta_p)\)</span> can be maximized (at least numerically).</p>
</div>
<div id="conditional-distributions-in-the-gaussian-case" class="section level3">
<h3><span class="header-section-number">11.1.3</span> Conditional distributions in the Gaussian case</h3>
<p>Assume first that the complete data <span class="math inline">\((X,Y)\)</span> has a multivariate normal distribution <span class="math inline">\(\mathcal{N}(\mu,\Sigma)\)</span>. The parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> may be estimated using maximum likelihood based procedures for incomplete data models such as the Expectation Maximization algorithm detailed above. Then, the conditional distribution of the missing data <span class="math inline">\(X_{\text{MIS}}\)</span> given the observations <span class="math inline">\(X_{\text{OBS}}\)</span> can be derived using Schur complements. If <span class="math inline">\(\Sigma_{\text{MIS}}\in\mathbb{R}^{m\times m}\)</span> is the covariance of <span class="math inline">\(X_{\text{MIS}}\)</span>, <span class="math inline">\(\Sigma_{\text{OBS}}\in\mathbb{R}^{p\times p}\)</span> is the covariance of <span class="math inline">\(X_{\text{OBS}}\)</span> and <span class="math inline">\(C_{\text{MIS},\text{OBS}}\in\mathbb{R}^{m\times p}\)</span> is the covariance matrix between <span class="math inline">\(X_{\text{MIS}}\)</span> and <span class="math inline">\(X_{\text{OBS}}\)</span> then <span class="math inline">\(\Sigma\)</span> is given by: <span class="math display">\[
\Sigma = \begin{pmatrix}
\Sigma_{\text{MIS}}&amp;C_{\text{MIS},\text{OBS}}\\C&#39;_{\text{MIS},\text{OBS}}&amp;\Sigma_{\text{OBS}}
\end{pmatrix}\,.
\]</span> Conditionally on <span class="math inline">\(X_{\text{OBS}}\)</span>, <span class="math inline">\(X_{\text{MIS}}\)</span> has a normal distribution with covariance matrix <span class="math inline">\(\Sigma_{X_{\text{MIS}}|X_{\text{OBS}}}\)</span> given by the Schur complement of <span class="math inline">\(\Sigma_{\text{OBS}}\)</span> in <span class="math inline">\(\Sigma\)</span>: <span class="math display">\[
\Sigma_{\text{MIS}|\text{OBS}} = \Sigma_{\text{MIS}} - C_{\text{MIS},\text{OBS}}\Sigma^{-1}_{\text{OBS}}C&#39;_{\text{MIS},\text{OBS}}\,.
\]</span></p>
<p>Note also that the mean <span class="math inline">\(\mu_{\text{MIS}|\text{OBS}}\)</span> of the distribution of <span class="math inline">\(X_{\text{MIS}}\)</span> given <span class="math inline">\(X_{\text{OBS}}\)</span> is: <span class="math display">\[
\mu_{\text{MIS}|\text{OBS}} = \mathbb{E}[X_{\text{MIS}}] + C_{\text{MIS},\text{OBS}}\Sigma_{\text{OBS}}^{-1}\left(X_{\text{OBS}} - \mathbb{E}[X_{\text{OBS}}]\right)\,.
\]</span></p>
<p>In R, we can estimate the mean and covariance matrix with EM and then impute missing values with the previous formulae with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(norm)
pre &lt;-<span class="st"> </span><span class="kw">prelim.norm</span>(<span class="kw">as.matrix</span>(don))
thetahat &lt;-<span class="st"> </span><span class="kw">em.norm</span>(pre)
<span class="kw">getparam.norm</span>(pre,thetahat)
imp &lt;-<span class="st"> </span><span class="kw">imp.norm</span>(pre,thetahat,don)</code></pre></div>
</div>
<div id="bootstrap" class="section level3">
<h3><span class="header-section-number">11.1.4</span> Bootstrap</h3>
<p>The bootstrap method is another way to estimate unknown parameters characterizing the statistical model: confidence intervals, estimation of the standard error etc. It is used when the unknown quantity to be estimated can be written as a functional of the unknwon distribution function <span class="math inline">\(f\)</span> of interest. For instance, in the case of incomplete data models, the bootstrap method is a solution to estimate any quantity which can be expressed as a functional of the unknown conditional distribution <span class="math inline">\(\pi\)</span> of the latent data <span class="math inline">\(X_{\text{MIS}}\)</span> given the observations.</p>
<p>Assume that <span class="math inline">\((X_i)_{1\le i \le n}\)</span> are i.i.d. with common unknown distribution function <span class="math inline">\(f\)</span> and let <span class="math inline">\(\theta\in\mathbb{R}^d\)</span> be any parameter characterizing <span class="math inline">\(f\)</span>. The parameter <span class="math inline">\(\theta\)</span> is estimated by <span class="math inline">\(\hat{\theta}(X_1,\ldots,X_n)\)</span>. Then, the variance of the estimator is given by: <span class="math display">\[
\mathbb{V}_f[\hat{\theta}(X_1,\ldots,X_n)] = \mathbb{E}_f\left[\left(\hat{\theta}(X_1,\ldots,X_n)-\mathbb{E}_f[\hat{\theta}(X_1,\ldots,X_n)]\right)^2\right]\,,
\]</span> where <span class="math inline">\(\mathbb{E}_f\)</span> is the expectation under the law of <span class="math inline">\((X_1,\ldots,X_n)\)</span>. The bootstrap estimator of <span class="math inline">\(\mathbb{V}_f[\hat{\theta}(X_1,\ldots,X_n)]\)</span> is obtained then by replacing the unknwon distribution function <span class="math inline">\(f\)</span> in this expression by its empirical estimate given, for any <span class="math inline">\(x\)</span>, by: <span class="math display">\[
f_n(x) = \frac{1}{n}\sum_{i=1}^n 1_{(-\infty,x]}(X_i)\,.
\]</span> For any integrable function <span class="math inline">\(h\)</span>, <span class="math display">\[
\mathbb{E}_{f_n}[h(Z)] = \frac{1}{n}\sum_{i=1}^nh(X_i)\,.
\]</span> Replacing <span class="math inline">\(f\)</span> by <span class="math inline">\(f_n\)</span> can lead to highly involved estimates but in some common situations the bootstrap estimate of <span class="math inline">\(\mathbb{V}_f[\hat{\theta}(X_1,\ldots,X_n)]\)</span> can be derived easily.</p>
<p>For instance, assume that <span class="math inline">\(\hat{\theta}(X_1,\ldots,X_n) = \bar{X}_n\)</span> is the empirical estimate of the mean of <span class="math inline">\(f\)</span>. Then, <span class="math display">\[
\mathbb{V}_f[\hat{\theta}(X_1,\ldots,X_n)] = \mathbb{V}_f[\bar{X}_n] = \frac{1}{n}\left(\mathbb{E}_f[X_1^2] - \mathbb{E}_f[X_1]^2\right)\,.
\]</span> Therefore, the bootstrap estimator of <span class="math inline">\(\mathbb{V}_f[\hat{\theta}(X_1,\ldots,X_n)]\)</span> is given by <span class="math display">\[
\mathbb{V}_{f_n}[\hat{\theta}(X_1,\ldots,X_n)]=\frac{1}{n}\left(\mathbb{E}_{f_n}[X_1^2] - \mathbb{E}_{f_n}[X_1]^2\right) = \frac{1}{n}\left(\frac{1}{n}\sum_{i=1}^nX_i^2 - \bar X_n^2\right)\,.
\]</span></p>
</div>
<div id="gibbs-sampling" class="section level3">
<h3><span class="header-section-number">11.1.5</span> Gibbs sampling</h3>
<p>In the case where the complete data <span class="math inline">\((X_{\text{OBS}},X_{\text{MIS}})\)</span> is not assumed to be a Gaussian vector, we may be interested in estimating or sampling from the (usually unknwon) conditional distribution <span class="math inline">\(\pi\)</span> of the missing data <span class="math inline">\(X\)</span> given the observations <span class="math inline">\(Y\)</span>. A widely spread technique to do so is to use Markov Chain Monte Carlo (MCMC) methods which naturally provide simulation based methods which have been successfully applied to many disciplines such as signal processing, biology, target tracking etc…<br />
One of the main objectives of these MCMC algorithms is to produce a Markov chain <span class="math inline">\((\xi^i)_{i\ge 0}\)</span> targetting the unknown target distribution <span class="math inline">\(\pi\)</span>. Using ergodic theory for Markov chains, it is expected for instance that <span class="math inline">\(N^{-1}\sum_{i=1}^N f(\xi^i)\)</span> is a good estimate of <span class="math display">\[
\int f(x)\pi(\mathrm{d}x) = \mathbb{E}[f(X_{\text{MIS}})|X_{\text{OBS}}]
\]</span> for a large class of functions <span class="math inline">\(f\)</span>.</p>
<p>Many MCMC algorithms have been developed to sample the chain <span class="math inline">\((\xi^i)_{i\ge 1}\)</span>, this section details the popular Gibbs sampler which may be used when the conditional distribution of each latent variable given all the other variables has a simple form. Assume that the missing data may be decomposed into several components <span class="math inline">\((X_1,\ldots,X_m)\)</span> and for all <span class="math inline">\(1\le k \le m\)</span> let <span class="math inline">\(\pi_{-k}\)</span> be the conditional distribution of <span class="math inline">\(X_k\)</span> given the observations and the other missing data. Then, starting with any initial state <span class="math inline">\(\xi^1 = (\xi^1_1,\ldots,\xi^1_m)\)</span>, for all <span class="math inline">\(i\ge 1\)</span>, conditionally on <span class="math inline">\(\xi^i\)</span>, the Gibss sampler samples <span class="math inline">\(\xi^{i+1}\)</span> as follows. For all <span class="math inline">\(1\le k \le m\)</span>, <span class="math inline">\(\xi^{i+1}_k\)</span> is sampled accodind to <span class="math inline">\(\pi_{-k}(\cdot |\xi^{i+1}_1,\ldots,\xi^{i+1}_{k-1},\xi^{i}_{k+1},\xi^{i}_{m})\)</span>. All components of the new state <span class="math inline">\(\xi^{i+1}\)</span> are sampled iteratively according to the conditional distribution given the observations and the other components. A nice feature of this conditional sampler is that at each iteration <span class="math inline">\(i\ge 1\)</span>, every component update <span class="math inline">\(1\le k \le m\)</span> (which produces <span class="math inline">\(\xi^{i+1}_k\)</span>) is reversible with respect to <span class="math inline">\(\pi\)</span> which implies that the Markov kernel associated with each component update admits <span class="math inline">\(\pi\)</span> as a stationary probability distribution. The Gibbs sampler convergence may be established for the complete update of all components at each iteration and several procedures have been proposed to combine the individual moves (not necessarily with a systematic update of each component in a row).</p>
</div>
</div>
<div id="lab-1" class="section level2">
<h2><span class="header-section-number">11.2</span> Lab</h2>
<div id="lecture-questions-1" class="section level3">
<h3><span class="header-section-number">11.2.1</span> Lecture questions</h3>
<ul>
<li><p>When you suggest methods to deal with missing values to users, the recurrent question is “What is the percentage of missing values that I can have in my data set, is 50% too much but 20% OK?” What is your answer to this question?</p></li>
<li><p>Explain the aims of multiple imputation in comparison to single imputation.</p></li>
</ul>
</div>
<div id="continuous-data-with-missing-values---regression-with-missing-data-via-multiple-imputation" class="section level3">
<h3><span class="header-section-number">11.2.2</span> Continuous data with missing values - Regression with missing data via Multiple Imputation</h3>
<p>First of all you will need to install the following packages</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="st">&quot;VIM&quot;</span>)
<span class="kw">install.packages</span>(<span class="st">&quot;missMDA&quot;</span>)
<span class="kw">install.packages</span>(<span class="st">&quot;Amelia&quot;</span>)</code></pre></div>
<p>Air pollution is currently one of the most serious public health worries worldwide. Many epidemiological studies have proved the influence that some chemical compounds, such as sulphur dioxide (SO2), nitrogen dioxide (NO2), ozone (O3) can have on our health. Associations set up to monitor air quality are active all over the world to measure the concentration of these pollutants. They also keep a record of meteorological conditions such as temperature, cloud cover, wind, etc.</p>
<p>We have at our disposal 112 observations collected during the summer of 2001 in Rennes. The variables available are</p>
<ul>
<li>maxO3 (maximum daily ozone)</li>
<li>maxO3v (maximum daily ozone the previous day)</li>
<li>T12 (temperature at midday)</li>
<li>T9</li>
<li>T15 (Temp at 3pm)</li>
<li>Vx12 (projection of the wind speed vector on the east-west axis at midday)</li>
<li>Vx9 and Vx15 as well as the Nebulosity (cloud) Ne9, Ne12, Ne15</li>
</ul>
<p>Here the final aim is to analyse the relationship between the maximum daily ozone (maxO3) level and the other meteorological variables. To do so we will perform regression to explain maxO3 in function of all the other variables. This data is incomplete (there are missing values). Indeed, it occurs frenquently to have machines that fail one day, leading to some information not recorded. We will therefore perform regression via multiple imputation.</p>
<ul>
<li>Import the data.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ozo &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;data/ozoneNA.csv&quot;</span>,<span class="dt">header=</span><span class="ot">TRUE</span>,
<span class="dt">sep=</span><span class="st">&quot;,&quot;</span>, <span class="dt">row.names=</span><span class="dv">1</span>)
WindDirection &lt;-<span class="st"> </span>ozo[,<span class="dv">12</span>]
don &lt;-<span class="st"> </span>ozo[,<span class="dv">1</span>:<span class="dv">11</span>]   #### keep the continuous variables
<span class="kw">summary</span>(don)</code></pre></div>
<pre><code>##      maxO3              T9             T12             T15       
##  Min.   : 42.00   Min.   :11.30   Min.   :14.30   Min.   :14.90  
##  1st Qu.: 71.00   1st Qu.:16.00   1st Qu.:18.60   1st Qu.:18.90  
##  Median : 81.50   Median :17.70   Median :20.40   Median :21.40  
##  Mean   : 91.24   Mean   :18.22   Mean   :21.46   Mean   :22.41  
##  3rd Qu.:108.25   3rd Qu.:19.90   3rd Qu.:23.60   3rd Qu.:25.65  
##  Max.   :166.00   Max.   :25.30   Max.   :33.50   Max.   :35.50  
##  NA&#39;s   :16       NA&#39;s   :37      NA&#39;s   :33      NA&#39;s   :37     
##       Ne9             Ne12            Ne15           Vx9         
##  Min.   :0.000   Min.   :0.000   Min.   :0.00   Min.   :-7.8785  
##  1st Qu.:3.000   1st Qu.:4.000   1st Qu.:3.00   1st Qu.:-3.0000  
##  Median :5.000   Median :5.000   Median :5.00   Median :-0.8671  
##  Mean   :4.987   Mean   :4.986   Mean   :4.60   Mean   :-1.0958  
##  3rd Qu.:7.000   3rd Qu.:7.000   3rd Qu.:6.25   3rd Qu.: 0.6919  
##  Max.   :8.000   Max.   :8.000   Max.   :8.00   Max.   : 5.1962  
##  NA&#39;s   :34      NA&#39;s   :42      NA&#39;s   :32     NA&#39;s   :18       
##       Vx12              Vx15            maxO3v      
##  Min.   :-7.8785   Min.   :-9.000   Min.   : 42.00  
##  1st Qu.:-3.6941   1st Qu.:-3.759   1st Qu.: 70.00  
##  Median :-1.9284   Median :-1.710   Median : 82.50  
##  Mean   :-1.6853   Mean   :-1.830   Mean   : 89.39  
##  3rd Qu.:-0.1302   3rd Qu.: 0.000   3rd Qu.:101.00  
##  Max.   : 6.5778   Max.   : 3.830   Max.   :166.00  
##  NA&#39;s   :10        NA&#39;s   :21       NA&#39;s   :12</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(don)</code></pre></div>
<pre><code>##          maxO3   T9  T12  T15 Ne9 Ne12 Ne15     Vx9    Vx12    Vx15 maxO3v
## 20010601    87 15.6 18.5   NA   4    4    8  0.6946 -1.7101 -0.6946     84
## 20010602    82   NA   NA   NA   5    5    7 -4.3301 -4.0000 -3.0000     87
## 20010603    92 15.3 17.6 19.5   2   NA   NA  2.9544      NA  0.5209     82
## 20010604   114 16.2 19.7   NA   1    1    0      NA  0.3473 -0.1736     92
## 20010605    94   NA 20.5 20.4  NA   NA   NA -0.5000 -2.9544 -4.3301    114
## 20010606    80 17.7 19.8 18.3   6   NA    7 -5.6382 -5.0000 -6.0000     94</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(don)</code></pre></div>
<pre><code>## [1] 112  11</code></pre>
<ul>
<li>Load the libraries.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(VIM)
<span class="kw">library</span>(FactoMineR)</code></pre></div>
<p>First, we perfom some descriptive statistics (how many missing? how many variables, individuals with missing?) and try to <strong>inspect and vizualize the pattern of missing entries and get hints on the mechanism</strong> that generated the missingness. For this purpose, we use the R package <strong>VIM</strong> (Visualization and Imputation of Missing Values - Mathias Templ) as well as Multiple Correspondence Analysis (FactoMineR package). The package VIM provides tools for the visualization of missing or imputed values, which can be used for exploring the data and the structure of the missing or imputed values. Depending on this structure, they may help to identify the mechanism generating the missing values or errors, which may have happened in the imputation process. You should install the package VIM, then you can check the documentation by executing</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">?VIM</code></pre></div>
<p>The VIM function <strong>aggr</strong> calculates and plots the amount of missing entries in each variables and in some combinations of variables (that tend to be missing simultaneously).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(<span class="kw">na.omit</span>(don))
res&lt;-<span class="kw">summary</span>(<span class="kw">aggr</span>(don, <span class="dt">sortVar=</span><span class="ot">TRUE</span>))$combinations</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(res[<span class="kw">rev</span>(<span class="kw">order</span>(res[,<span class="dv">2</span>])),])
<span class="kw">aggr</span>(don, <span class="dt">sortVar=</span><span class="ot">TRUE</span>)</code></pre></div>
<p>The VIM function <strong>matrixplot </strong> creates a matrix plot in which all cells of a data matrix are visualized by rectangles. Available data is coded according to a continuous color scheme (gray scale), while missing/imputed data is visualized by a clearly distinguishable color (red). If you use Rstudio the plot is not interactive (thus the warnings), but if you use R directly, you can click on a column of your choice, this will result in sorting the rows in the decreasing order of the values of this column. This is useful to check if there is an association between the value of a variable and the missingness of another one.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">matrixplot</span>(don,<span class="dt">sortby=</span><span class="dv">2</span>) <span class="co"># marche pas sur Rstudio</span></code></pre></div>
<p>The VIM function <strong>marginplot</strong> creates a scatterplot with additional information on the missing values. If you plot the variables (x,y), the points with no missing values are represented as in a standard scatterplot. The points for which x (resp. y) is missing are represented in red along the y (resp. x) axis. In addition, boxplots of the x and y variables are represented along the axes with and without missing values (in red all variables x where y is missing, in blue all variables x where y is observed).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">marginplot</span>(don[,<span class="kw">c</span>(<span class="st">&quot;T9&quot;</span>,<span class="st">&quot;maxO3&quot;</span>)])</code></pre></div>
<ul>
<li><p>Do you observe any associations between the missing entries ? When values are missing on a variable does it correspond to small or large values on another one ? (For this question you need to use the matrixplot function in R)</p></li>
<li><p>Create a categorical dataset with “o” when the value of the cell is observed and “m” when it is missing, and with the same row and column names as in the original data. Then, you can perform Multiple Correspondence Analysis to visualize the association with the <strong>MCA</strong> function.</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">?MCA</code></pre></div>
<p>Then, before modeling the data, we perform a <strong>PCA with missing values</strong> to explore the correlation between variables. Use the R package <strong>missMDA</strong> dedicated to perform principal components methods with missing values and to impute data with PC methods.</p>
<ul>
<li>Determine the number of components ncp to keep using the <strong>estim_ncpPCA</strong> function. Perform PCA with missing values using the <strong>imputePCA</strong> function and ncp components. Then plot the correlation circle.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">?estim_ncpPCA
?imputePCA</code></pre></div>
<ul>
<li>Could you guess how cross-validation is performed to select the number of components?</li>
</ul>
<p>Then, to run the regression with missing values, we use <strong>Multiple Imputation</strong>. We impute the data either assuming 1) a Gaussian distribution (library Amelia) or 2) a PCA based model (library missMDA). Note that there are two ways to impute either using a Joint Modeling (one joint probabilitisc model for the variables all together) or a Condional Modeling (one model per variable) approach. We refer to the references given in the slides for more details. We use the R package <strong>Amelia</strong>. We generate 100 imputed data sets with the amelia method:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Amelia)</code></pre></div>
<pre><code>## Loading required package: Rcpp</code></pre>
<pre><code>## ## 
## ## Amelia II: Multiple Imputation
## ## (Version 1.7.4, built: 2015-12-05)
## ## Copyright (C) 2005-2017 James Honaker, Gary King and Matthew Blackwell
## ## Refer to http://gking.harvard.edu/amelia/ for more information
## ##</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">?amelia</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res.amelia &lt;-<span class="st"> </span><span class="kw">amelia</span>(don, <span class="dt">m=</span><span class="dv">100</span>)  
<span class="co">#names(res.amelia$imputations) </span>
res.amelia$imputations$imp1<span class="co"># the first imputed data set</span></code></pre></div>
<ul>
<li>Now generate 100 imputed data sets with the MIPCA method and 2 components. Store the result in a variable called res.MIPCA.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">?MIPCA
?plot.MIPCA</code></pre></div>
<p>We will <strong>inspect the imputed values created</strong> to know if the imputation method should require more investigation or if we can continue and analyze the data. A common practice consists in comparing the distribution of the imputed values and of the observed values. Check the <strong>compare.density</strong> function and apply it to compare the distributions of the T12 variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">?compare.density</code></pre></div>
<ul>
<li>Do both distributions need to be close? Could the missing values differ from the observed ones both in spread and in location?</li>
</ul>
<p>The quality of imputation can also be assessed with cross-validation using the <strong>overimpute</strong> function. Each observed value is deleted and for each one 100 values are predicted (using the same MI method) and the mean and 90% confidence intervals are computed for these 100 values. Then, we inspect whether the observed value falls within the obtained interval. On the graph, the y=x line is plotted (where the imputations should fall if they were perfect), as well as the mean (dots) and intervals (lines) for each value. Around ninety percent of these confidence intervals should contain the y = x line, which means that the true observed value falls within this range. The color of the line (as coded in the legend) represents the fraction of missing observations in the pattern of missingness for that observation (ex: blue=0-2 missing entries).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">?overimpute</code></pre></div>
<ul>
<li>Comment the quality of the imputation.</li>
</ul>
<p>We can also examine the variability by projecting as supplementary tables the imputed data sets on the PCA configuration (plot the results of MI with PCA).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(res.MIPCA,<span class="dt">choice=</span> <span class="st">&quot;ind.supp&quot;</span>)
<span class="kw">plot</span>(res.MIPCA,<span class="dt">choice=</span> <span class="st">&quot;var&quot;</span>)</code></pre></div>
<ul>
<li>Apply a regression model on each imputed data set of the amelia method. Hint: a regression with several variables can be performed as follows ‘lm(formula=“maxO3 ~ T9+T12”, data =don)’. You can also use the function <strong>with</strong>.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">resamelia &lt;-<span class="st"> </span><span class="kw">lapply</span>(res.amelia$imputations, as.data.frame)
<span class="co"># A regression on each imputed dataset</span>
fitamelia&lt;-<span class="kw">lapply</span>(resamelia, lm, 
                  <span class="dt">formula=</span><span class="st">&quot;maxO3~ T9+T12+T15+Ne9+Ne12+Ne15+Vx9+Vx12+Vx15+maxO3v&quot;</span>)  
<span class="co"># fitamelia &lt;- lapply(resamelia, with, </span>
<span class="co">#                     lm(maxO3 ~ T9+T12+T15+Ne9+Ne12+Ne15+Vx9+Vx12+Vx15+maxO3v))</span></code></pre></div>
<ul>
<li>Now do the same with the imputed datasets of the MIPCA method.</li>
</ul>
<p>The package <strong>mice</strong> (Multiple Imputation by Chained Equations) allows to aggregate the results from some simple models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mice)</code></pre></div>
<pre><code>## Loading required package: lattice</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ?mice</span>
<span class="co"># pool is a function from mice to aggregate the results according to Rubin&#39;s rule</span>
<span class="co"># ?pool</span></code></pre></div>
<ul>
<li>Aggregate the results of Regression with Multiple Imputation according to Rubin’s rule (slide “Multiple imputation”) for MI with amelia with the <strong>pool</strong> function from the mice package.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">poolamelia&lt;-<span class="kw">pool</span>(<span class="kw">as.mira</span>(fitamelia)) 
<span class="kw">summary</span>(poolamelia)</code></pre></div>
<ul>
<li><p>Now do the same with the MIPCA results.</p></li>
<li><p>Write a function that removes the variables with the largest pvalues step by step (each time a variable is removed the regression model is performed again) until all variables are significant.</p></li>
</ul>
</div>
<div id="em-algorithm-for-bivariate-normal-data-with-missing-values" class="section level3">
<h3><span class="header-section-number">11.2.3</span> EM algorithm for bivariate normal data with missing values</h3>
<p>author: “Julie Josse - Nicolas Brosse - Geneviève Robin” Exam 2016</p>
<p>The purpose of this problem is to use the EM algorithm to estimate the mean of a bivariate normal dataset with missing entries in one of the two variables. We first generate synthetic data and then implement the EM algorithm to compute the estimator of the mean.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mvtnorm)</code></pre></div>
<p>We consider a bivariate normal random variable <span class="math inline">\(y=\begin{pmatrix} y_1\\ y_2 \end{pmatrix}\)</span> and denote the mean vector and covariance matrix of its distribution <span class="math inline">\(\mu=\begin{pmatrix} \mu_1\\ \mu_2 \end{pmatrix}\)</span> and <span class="math inline">\(\Sigma=\begin{pmatrix} \sigma_{11} &amp; \sigma_{12}\\ \sigma_{12} &amp; \sigma_{22} \end{pmatrix}\)</span>: <span class="math inline">\(y\sim \mathcal{N}(\mu, \Sigma)\)</span>. We observe a sample of size <span class="math inline">\(n\)</span> that contains some missing values in the variable <span class="math inline">\(y_2\)</span>, such that for some <span class="math inline">\(r\leq n\)</span>, we observe <span class="math inline">\((y_{i1},y_{i2})\)</span> for <span class="math inline">\(i=1,..., r\)</span> and <span class="math inline">\(y_{i1}\)</span> for <span class="math inline">\(i=r+1,... n\)</span>. The goal is to estimate the mean <span class="math inline">\(\mu\)</span>. We will compare two strategies: 1) direct computation of the maximum likelihood estimator and 2) estimation of the mean with the EM algorithm.</p>
<div id="data-generation" class="section level4">
<h4><span class="header-section-number">11.2.3.1</span> Data generation</h4>
<p><strong>(R1)</strong> Generate a bivariate normal sample of size <span class="math inline">\(100\)</span> of mean <span class="math inline">\(\begin{pmatrix} \mu_1\\ \mu_2 \end{pmatrix}=\begin{pmatrix} 5\\ -1 \end{pmatrix}\)</span> and covariance matrix <span class="math inline">\(\begin{pmatrix} \sigma_{11} &amp; \sigma_{12}\\ \sigma_{12} &amp; \sigma_{22} \end{pmatrix}=\begin{pmatrix} 1.3 &amp; 0.4\\ 0.4 &amp; 0.9 \end{pmatrix}\)</span> containing 30% of missing values in the variable <span class="math inline">\(y_2\)</span>.</p>
</div>
<div id="maximum-likelihood-estimator" class="section level4">
<h4><span class="header-section-number">11.2.3.2</span> Maximum likelihood estimator</h4>
<p>We denote by <span class="math inline">\(f_{1,2}(y_1,y_2;\mu, \Sigma)\)</span>, <span class="math inline">\(f_1(y_1;\mu_1, \sigma_{11})\)</span> and <span class="math inline">\(f_{2|1}(y_2|y_1; \mu, \Sigma)\)</span> the probability density functions of the joint <span class="math inline">\((y_1,y_2)\)</span>, <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2|y_1\)</span> respectively. The likelihood of the observed data can be written as <span class="math display">\[f_{1,2}(y_1,y_2;\mu, \Sigma)=\prod_{i=1}^nf_1(y_{i1})\prod_{j=1}^rf_{2|1}(y_{j2}|y_{j1}),\]</span> and the log-ikelihood is written (up to an additional constant that does not appear in the maximization and that we therefore drop)</p>
<p><span class="math display">\[l(\mu, \Sigma|y_1,y_2)=-\frac{n}{2}\log(\sigma_{11}^2)-\frac{1}{2}\sum_{i=1}^n\frac{(y_{i1}-\mu_1)^2}{\sigma_{11}^2}-\frac{r}{2}\log((\sigma_{22}-\frac{\sigma_{12}^2}{\sigma_{11}})^2)\]</span> <span class="math display">\[-\frac{1}{2}\sum_{i=1}^r\frac{(y_{i2}-\mu_2-\frac{\sigma_{12}}{\sigma_{11}}(y_{i1}-\mu_1))^2}{(\sigma_{22}-\frac{\sigma_{12}^2}{\sigma_{11}})^2}\]</span></p>
<p>We skip the computations and directly give the expression of the closed form maximum likelihood estimator of the mean: <span class="math display">\[  \hat{\mu}_1=n^{-1}\sum_{i=1}^ny_{i1} \]</span> <span class="math display">\[  \hat{\mu}_2=\hat{\beta}_{20.1}+\hat{\beta}_{21.1}\hat{\mu}_1,\]</span></p>
<p><span class="math inline">\(\hat{\beta}_{21.1}=s_{12}/s_{11}\)</span>, <span class="math inline">\(\hat{\beta}_{20.1}=\bar{y}_2-\hat{\beta}_{21.1}\bar{y}_1\)</span>, and <span class="math inline">\(\bar{y}_j=r^{-1}\sum_{i=1}^ry_{ij}\)</span>, <span class="math inline">\(j=1,2\)</span> and <span class="math inline">\(s_{jk}=r^{-1}\sum_{i=1}^r(y_{ij}-\bar{y}_j)(y_{ik}-\bar{y}_k)\)</span>, <span class="math inline">\(j,k=1,2\)</span></p>
<p><strong>(R2)</strong> Compute the maximum likelihood estimates of <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span>.</p>
</div>
<div id="em-algorithm" class="section level4">
<h4><span class="header-section-number">11.2.3.3</span> EM algorithm</h4>
<p>In this simple setting, we have an explicit expression of the maximum likelihood estimator despite missing values. However, this is not always the case but it is possible to use an EM algorithm which allows to get the maximum likelihood estimators in the cases where data are missing.</p>
<p>The EM algorithm consists in maximizing the “observed likelihood” <span class="math display">\[l(\mu, \Sigma|y_1,y_2)=-\frac{n}{2}\log(\sigma_{11}^2)-\frac{1}{2}\sum_{i=1}^n\frac{(y_{i1}-\mu_1)^2}{\sigma_{11}^2}-\frac{r}{2}\log((\sigma_{22}-\frac{\sigma_{12}^2}{\sigma_{11}})^2)\]</span> <span class="math display">\[-\frac{1}{2}\sum_{i=1}^r\frac{(y_{i2}-\mu_2-\frac{\sigma_{12}}{\sigma_{11}}(y_{i1}-\mu_1))^2}{(\sigma_{22}-\frac{\sigma_{12}^2}{\sigma_{11}})^2},\]</span> through successive maximization of the “complete likelihood” (if we had observed all <span class="math inline">\(n\)</span> realizations of <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span>). Maximizing the complete likelihood <span class="math display">\[l_c(\mu, \Sigma|y_1,y_2)=-\frac{n}{2}\log(\det(\Sigma))-\frac{1}{2}\sum_{i=1}^n(y_{i1}-\mu_1)^T\Sigma^{-1}(y_{i1}-\mu_1)\]</span></p>
<p>would be straightforward if we had all the observations. However elements of this likelihood are not available. Consequently, we replace them by the conditional expectation given observed data and the parameters of the current iteration. These two steps of computation of the conditional expectation (E-step) and maximization of the completed likelihood (M step) are repeated until convergence.</p>
<p>The update formulas for the E and M steps are the following</p>
<p><strong>E step</strong>:</p>
<p>The sufficient statistics of the likelihood are:</p>
<p><span class="math display">\[s_1=\sum_{i=1}^ny_{i1},\quad s_2=\sum_{i=1}^ny_{i2},\quad s_{11}=\sum_{i=1}^ny_{i1}^2,\quad s_{22}=\sum_{i=1}^ny_{i2}^2,\quad s_{12}=\sum_{i=1}^ny_{i1}y_{i2}.\]</span></p>
<p>Since some values of <span class="math inline">\(y_2\)</span> are not available, we fill in the sufficient statistics with:</p>
<p><span class="math display">\[E[y_{i2}|y_{i1},\mu,\Sigma]=\beta_{20.1}+\beta_{21.1}y_{i1}\]</span> <span class="math display">\[E[y_{i2}^2|y_{i1},\mu,\Sigma]=(\beta_{20.1}+\beta_{21.1}y_{i1})^2+\sigma_{22.1}\]</span> <span class="math display">\[E[y_{i2}y_{i2}|y_{i1},\mu,\Sigma]=(\beta_{20.1}+\beta_{21.1}y_{i1})y_{i1}.\]</span></p>
<p><strong>M step</strong>:</p>
<p>The M step consists in computing the maximum likelihood estimates as usual. Given <span class="math inline">\(s_1, s_2, s_{11}, s_{22}, \text{and } s_{12},\)</span> update <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\sigma}\)</span> with <span class="math display">\[\hat{\mu}_1=s_1/n\text{, }\hat{\mu}_2=s_2/n,\]</span> <span class="math display">\[\hat{\sigma}_1=s_{11}/n-\hat{\mu}_1^2\text{, }\hat{\sigma}_2=s_{22}/n-\hat{\mu}_2^2\text{, }\hat{\sigma}_{12}=s_{12}/n-\hat{\mu}_1\hat{\mu}_2\]</span></p>
<p>Note that <span class="math inline">\(s_1\)</span>, <span class="math inline">\(s_{11}\)</span>, <span class="math inline">\(\hat{\mu}_1\)</span> and <span class="math inline">\(\hat{\sigma}_1\)</span> are constant accross iterations since we do not have missing values on <span class="math inline">\(y_1\)</span>.</p>
<p><strong>(R3)</strong> Write two functions called Estep and Mstep that respectively perform the E step and the M step. The Estep function can take as an input <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span>. Then, you can compute <span class="math inline">\(\beta_{21.1}=\sigma_{12}/\sigma_{11}\)</span>, <span class="math inline">\(\beta_{20.1}=\mu_2-\beta_{21.1}\mu_1\)</span>, and <span class="math inline">\(\sigma_{22.1}=\sigma_{22}-\sigma^2_{12}/\sigma_{11}\)</span> and update the sufficient statistics <span class="math inline">\(s_{ij}\)</span>. The Mstep function consists in updating the update the <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> given the <span class="math inline">\(s_{ij}\)</span>.</p>
<p><strong>(Q1)</strong> How could we initialize the algorithm ?</p>
<p><strong>(R4)</strong> Implement a function called initEM that returns initial values for <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span>.</p>
<p><strong>(R5)</strong> Implement the EM algorithm over 15 iterations and plot the value of <span class="math inline">\(\left\|\mu-\hat{\mu}\right\|^2\)</span> over iterations. Comment your results briefly.</p>
<p><strong>(R6)</strong> Check that the EM estimator <span class="math inline">\(\mu\)</span> is equal to the maximum likelihood estimator.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="homework-pca.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="shiny-app.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/10-Missing.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
